在《Stronger Together》这篇论文中，马尔可夫博弈（Markov Game）的定义是**整个多智能体系统的数学框架**。它不是我们熟悉的、智能体与环境单步互动的简单MDP，而是专门为**描述多个智能体在共享环境中交替协作**而设计的。

为了让你直观地理解它和传统MDP的区别，我把它拆解为**“谁、看到了什么、做了什么、得到了什么、环境怎么变”**这五个要素，对应论文中的形式化定义：

> $\mathcal{M} = (\mathcal{S}, \{\mathcal{A}_i\}_{i=1}^N, \mathcal{T}, \{r_i\}_{i=1}^N, T, H)$

以下是每个符号的**真实含义**及其在MAS+RL场景下的具体映射：

### 1. 状态空间 $\mathcal{S}$ —— 环境的“全景快照”
*   **它记录什么**：当前关卡的地图布局、代码问题的题目描述、数学题的文本，以及**所有智能体迄今为止的完整对话历史**。
*   **关键点**：在MAS中，$S$ 不仅包含环境本身，还包含智能体之间**说过什么话**。

### 2. 动作空间 $\mathcal{A}_i$ —— 第 $i$ 个智能体“能做的事”
*   **论文特指**：这里的一个动作 $a_{i,t}$ **不是单个token**，而是**一次完整的LLM生成结果**（论文称为“宏动作”macro-action）。例如：
    *   规划智能体输出一整套代码方案；
    *   测试智能体输出一个单元测试用例。
*   **核心差异**：传统RL动作是低维的（左/右），这里是**高维、变长的自然语言序列**。

### 3. 转移函数 $\mathcal{T}$ —— “对话轮次”的推进器
*   **公式**：$s_{t+1} = \mathcal{T}(s_t, a_{1,t}, ..., a_{N,t})$
*   **含义**：所有智能体在**本轮都说完话**后，环境才会更新。
*   **举例**：在代码生成任务中，$s_t$ 是“题目+历史讨论”，程序员输出代码（$a_{1,t}$），测试员输出测试用例（$a_{2,t}$）。$\mathcal{T}$ 执行**编译运行**，得到执行结果，并将这个结果**追加到状态** $s_{t+1}$ 中，交给下一轮。

### 4. 奖励函数 $r_i$ —— 谁负责领赏/担责
*   **公式**：$r_i: \mathcal{A}_i \to [0,1]$
*   **设计逻辑**：这是**AT-GRPO算法的核心改良点**。
    *   在**角色共享策略**中：所有智能体共享最终任务的奖励（同甘共苦）。
    *   在**角色专精策略**中：智能体获得**过程奖励**。例如，“测试员”只要写出**语法正确、能运行**的测试用例，即使被测代码是错的，它也能拿到**正奖励**。
*   **目的**：解决**信用分配（Credit Assignment）**问题——避免“代码写错了，连累写测试用例的人也受罚”。

### 5. 两个时间尺度：$T$ 和 $H$
*   **$T$ —— 回合长度**：一局游戏最多允许多少轮对话，或者环境的最大步数。
*   **$H$ —— 优化步长**：每收集多少轮数据，做一次策略更新。这是**在线策略**的标志：边交互、边学习。

---

### 为什么要在MAS中用这个定义？

这个定义解决了**单智能体RL框架无法处理多智能体**的两个根本问题：

1.  **动作的异步性**：它不是每一步只动一个人，而是**允许在一个回合内所有智能体依次发言**，然后统一更新状态。这符合现实中的会议讨论模式。
2.  **策略的独立性**：通过 $\sigma: \{1,...,N\} \to \{1,...,M\}$ 这个映射，**同一个LLM可以扮演多个角色**（角色共享），**也可以每个角色都有自己的LLM**（角色专精）。$\mathcal{M}$ 的定义框架都能覆盖。

**总结**：这篇论文的马尔可夫博弈定义，本质上是在说——**多智能体协作写代码/做规划，本质上是一个“轮流发言、根据对话历史决定下一句说什么、最后根据整体结果给每个人打分”的序列决策过程**。AT-GRPO算法正是基于这套流程，来设计“按角色、按轮次分组”的优势计算方法。